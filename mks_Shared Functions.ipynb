{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1cd45f4a-c7c0-45ee-b370-bf5f33b56447",
      "cell_type": "markdown",
      "source": "# Shared Functions",
      "metadata": {}
    },
    {
      "id": "eb06a692-2e2c-4f34-8d1c-4ca8604302ee",
      "cell_type": "code",
      "source": "import os\nimport time\nimport logging\nfrom datetime import datetime\nfrom functools import reduce\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import (\n    broadcast,\n    col,\n    coalesce,\n    concat,\n    current_timestamp,\n    date_sub,\n    first,\n    lead,\n    lit,\n    lower,\n    regexp_replace,\n    sum as _sum,\n    trim,\n    when,\n)\nfrom pyspark.sql.types import DateType, DecimalType\nfrom pyspark.sql.window import Window\n\n\n# Function to retrieve the Bronze storage path dynamically based on the current workspace\ndef get_se_bronze_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Bronze lakehouse paths\n    workspace_lakehouse_dict = {\n        'DevSecure_Enclave': 'abfss://DevSecure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Bronze.Lakehouse/Tables',\n        'Secure_Enclave': 'abfss://Secure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Bronze.Lakehouse/Tables'\n    }\n    \n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\ndef get_se_pewter_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Bronze lakehouse paths\n    workspace_lakehouse_dict = {\n        'DevSecure_Enclave': 'abfss://DevSecure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Pewter.Lakehouse/Tables',\n        'Secure_Enclave': 'abfss://Secure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Pewter.Lakehouse/Tables'\n    }\n    \n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n# Function to retrieve the Silver storage path dynamically based on the current workspace\ndef get_se_silver_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n\n    workspace_lakehouse_dict = {\n        'DevSecure_Enclave': 'abfss://DevSecure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Silver.Lakehouse/Tables',\n        'Secure_Enclave': 'abfss://Secure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Silver.Lakehouse/Tables'\n        }\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n\ndef get_se_gold_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Silver lakehouse paths\n    workspace_lakehouse_dict = {\n        'DevSecure_Enclave': 'abfss://DevSecure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Gold.Lakehouse/Tables', \n        'Secure_Enclave': 'abfss://Secure_Enclave@onelake.dfs.fabric.microsoft.com/V6_SE_Gold.Lakehouse/Tables'\n    }\n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n\n# Function to retrieve the Bronze storage path dynamically based on the current workspace\ndef get_bronze_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Bronze lakehouse paths\n    workspace_lakehouse_dict = {\n        'DevSecure_Enclave': 'abfss://DevBronze@onelake.dfs.fabric.microsoft.com/BronzeLake.Lakehouse/Tables',\n        'Secure_Enclave': 'abfss://ProdBronze@onelake.dfs.fabric.microsoft.com/BronzeLake.Lakehouse/Tables'\n    }\n    \n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\ndef get_pewter_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Bronze lakehouse paths\n    workspace_lakehouse_dict = {\n            'DevSecure_Enclave': 'abfss://DevPewter@onelake.dfs.fabric.microsoft.com/PewterPreProcessed.Lakehouse/Tables', \n            'Secure_Enclave': 'abfss://ProdPewter@onelake.dfs.fabric.microsoft.com/PewterPreProcessed.Lakehouse/Tables',\n    }\n    \n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n# Function to retrieve the Silver storage path dynamically based on the current workspace\ndef get_silver_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    workspace_lakehouse_dict = {\n            'DevSecure_Enclave': 'abfss://DevSilver@onelake.dfs.fabric.microsoft.com/SilverTransformed.Lakehouse/Tables', \n            'Secure_Enclave': 'abfss://ProdSilver@onelake.dfs.fabric.microsoft.com/SilverTransformed.Lakehouse/Tables'\n        }\n    \n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n\ndef get_gold_path():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Silver lakehouse paths\n    workspace_lakehouse_dict = {\n            'DevSecure_Enclave': 'abfss://DevGold@onelake.dfs.fabric.microsoft.com/DevGold_Lake.Lakehouse/Tables', \n            'Secure_Enclave': 'abfss://34c8d809-ab93-4e32-acb3-911afc62b7f9@onelake.dfs.fabric.microsoft.com/7a6791f6-1a48-43e1-80f7-019e4be82bcc/Tables'\n    }\n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found\n\n\n# Function to save a DataFrame as a Delta table in either the Silver or Bronze layer\ndef save_table(df, transformed_table_name, layer):\n    spark.conf.set('spark.sql.caseSensitive', True)  # Ensure case sensitivity in column names\n    \n    # Determine destination storage path based on the specified layer\n    if layer.lower() == \"silver\":\n        destination_base_path = get_silver_path()\n    elif layer.lower() == \"bronze\":\n        destination_base_path = get_bronze_path()\n    elif layer.lower() == \"pewter\":\n        destination_base_path = get_pewter_path()\n    else:\n        raise ValueError(\"Invalid layer specified. Use 'bronze', 'pewter', or 'silver'.\")\n\n    # If the storage path cannot be determined, raise an error\n    if destination_base_path is None:\n        raise RuntimeError(f\"Could not determine the {layer} storage path.\")\n\n    # Construct the full destination path\n    destination_path = f\"{destination_base_path}/{transformed_table_name}\"\n\n    # Save the DataFrame as a Delta table\n    df.write.format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .option(\"overwriteSchema\", \"true\") \\\n        .save(destination_path)\n\n    print(f\"Table '{transformed_table_name}' saved to {destination_path}\")\n\n# Function to retrieve table data using Spark SQL\ndef get_table_data_sql(table_name, selected_columns=None, database=\"dbo\"):\n    table = f\"{database}.{table_name}\"  # Define the fully qualified table name\n    columns_str = \", \".join(selected_columns) if selected_columns else \"*\"  # Select specified columns or all columns\n    query = f\"SELECT {columns_str} FROM {table}\"  # Construct the SQL query\n    \n    return spark.sql(query)  # Execute the query and return the result as a DataFrame\n\n# Function to retrieve table data using Spark's read.load() instead of SQL\ndef get_table_data(table_name, lakehouse_path, selected_columns=None):\n    table_path = f\"{lakehouse_path}/{table_name}\"  # Construct the table path\n    df = spark.read.format('delta').load(table_path)  # Read the data from storage\n\n    # Select only the specified columns if provided\n    if selected_columns:\n        df = df.select(*selected_columns)\n    \n    return df  # Return the DataFrame\n\n# Function to setup logger capabilities\ndef setup_logger(log_name):\n    \"\"\"\n    Sets up a logger with both console and file handlers.\n\n    :param log_name: Name of the log (used for log directory and filename)\n    :return: Configured logger instance\n    \"\"\"\n    log_path = \"/lakehouse/default/Files/Logs\"\n    logging_path = os.path.join(log_path, log_name)\n\n    # Ensure the log directory exists\n    os.makedirs(logging_path, exist_ok=True)\n\n    # Create log file name with timestamp\n    log_filename = f\"{log_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n    log_file_path = os.path.join(logging_path, log_filename)\n\n    # Create a logger with the given name\n    logger = logging.getLogger(log_name)\n    logger.setLevel(logging.INFO)\n\n    # Remove existing handlers to avoid duplicates\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    # Console handler\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.INFO)\n    stream_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    stream_handler.setFormatter(stream_formatter)\n    logger.addHandler(stream_handler)\n\n    # File handler\n    file_handler = logging.FileHandler(log_file_path, mode='a')\n    file_handler.setLevel(logging.INFO)\n    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(file_formatter)\n    logger.addHandler(file_handler)\n\n    logger.info(f\"{log_name} logging setup complete.\")\n\n    return logger  # Return the logger for direct use\n\n# Function to move tables based on prefix to Gold Lakehouse - dynamically adjusts on Dev and Prod\ndef move_tables_to_gold(\n    prefix,\n):\n    \"\"\"\n    Transfers tables from a Silver (transformed) path to a Gold path if their names start with a given prefix.\n\n    Parameters:\n    - silver_path (str): Source path where transformed Delta tables are stored.\n    - gold_path (str): Destination path for Gold layer Delta tables.\n    - prefix (str): Prefix used to identify which tables to move.\n    - spark_session (SparkSession, optional): The Spark session to use.\n    - logger (Logger, optional): Logger instance to capture logs.\n    \"\"\"\n    silver_path = get_silver_path()\n    gold_path = get_gold_path()\n\n    if logger:\n        logger.info(f'Starting transfer of tables with prefix \"{prefix}\" from Silver ({silver_path}) to Gold ({gold_path}).')\n\n    tables_df = mssparkutils.fs.ls(silver_path)\n    table_names = [entry.name for entry in tables_df if entry.name.startswith(prefix)]\n\n    for table in table_names:\n        try:\n            df = spark.read.format(\"delta\").load(f\"{silver_path}/{table}\")\n            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{table}\")\n            \n            if logger:\n                logger.info(f\"Successfully moved {table} to {gold_path}\")\n        except Exception as e:\n            if logger:\n                logger.error(f\"Failed to move {table} to Gold: {str(e)}\")\n            else:\n                print(f\"Failed to move {table}: {e}\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d7e9aafa-b74f-45ea-8a4c-0054f85c86dd",
      "cell_type": "markdown",
      "source": "### Guardrail Functions",
      "metadata": {}
    },
    {
      "id": "79514653-adf7-4e69-bf3b-29b25a13cd30",
      "cell_type": "code",
      "source": "def validate_row_count(denorm_df, fact_df):\n    fact_count = fact_df.count()\n    denorm_count = denorm_df.count()\n\n    if denorm_count != fact_count:\n        raise ValueError(f\"âŒ Row count validation check failed: fact df has {fact_count} rows, while denorm_df has {denorm_count} rows.\")\n\n    print(f\"âœ… Row count validation check passed: {fact_count} rows.\")\n    return True\n\ndef validate_duplicates(denorm_df, columns):\n    duplicate_count = denorm_df.groupBy(columns).count().filter(\"count > 1\").count()\n\n    if duplicate_count > 0:\n        raise ValueError(f\"âŒ Duplicate check failed: found {duplicate_count} duplicate rows based on columns {columns}.\")\n\n    print(f\"âœ… Duplicates validation check passed for columns: {columns}.\")\n    return True\n\ndef validate_nulls(df, columns):\n    if isinstance(columns, str):\n        columns = [columns]\n\n    null_counts = df.select([\n        F.count(F.when(F.col(col).isNull(), col)).alias(col) for col in columns\n    ]).collect()[0].asDict()\n\n    failing_columns = {col: count for col, count in null_counts.items() if count > 0}\n\n    if failing_columns:\n        raise ValueError(f\"âŒ Null check failed: found nulls in columns: {failing_columns}\")\n\n    print(f\"âœ… Nulls validation check passed for columns: {columns}.\")\n    return True\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b3cab474-d2e8-4837-8458-a948377b490e",
      "cell_type": "markdown",
      "source": "### Function to display info on all the functions inside SharedFunctions Notebook",
      "metadata": {}
    },
    {
      "id": "59354900-3ea4-4ea5-b6e9-814cdb1c813e",
      "cell_type": "code",
      "source": "def shared_function_details():\n    functions_info = {\n        \"get_bronze_path\": {\n            \"purpose\": \"Retrieves the Bronze storage path dynamically based on the current workspace.\",\n            \"parameters\": \"None\",\n            \"returns\": \"The Bronze storage path as a string or None if not found.\"\n        },\n        \"get_silver_path\": {\n            \"purpose\": \"Retrieves the Silver storage path dynamically based on the current workspace.\",\n            \"parameters\": \"type (str): 'raw' for raw Silver data, any other value for transformed Silver data.\",\n            \"returns\": \"The Silver storage path as a string or None if not found.\"\n        },\n        \"save_table\": {\n            \"purpose\": \"Saves a Spark DataFrame as a Delta table in either the Silver or Bronze layer.\",\n            \"parameters\": \"\"\"\n                df (DataFrame): The Spark DataFrame to save.\n                transformed_table_name (str): The name of the table.\n                layer (str): The storage layer ('silver' or 'bronze').\"\"\",\n            \"returns\": \"None (Saves the DataFrame and prints confirmation).\"\n        },\n        \"get_table_data_sql\": {\n            \"purpose\": \"Retrieves table data using Spark SQL.\",\n            \"parameters\": \"\"\"\n                table_name (str): The name of the table.\n                selected_columns (list, optional): A list of column names to select. Defaults to None (selects all columns).\n                database (str, optional): The database schema. Defaults to 'dbo'.\"\"\",\n            \"returns\": \"A Spark DataFrame containing the queried data.\"\n        },\n        \"get_table_data\": {\n            \"purpose\": \"Retrieves table data using Spark's read.load() instead of SQL.\",\n            \"parameters\": \"\"\"\n                table_name (str): The name of the table.\n                lakehouse_path (str): The base path of the lakehouse.\n                type (str, optional): Specifies whether to get 'raw' or transformed Silver data. Defaults to 'raw'.\n                selected_columns (list, optional): A list of columns to select. Defaults to None (selects all columns).\"\"\",\n            \"returns\": \"A Spark DataFrame containing the retrieved data.\"\n        }\n    }\n\n    for func_name, details in functions_info.items():\n        print(f\"\\nFunction: {func_name}\")\n        print(f\"  Purpose: {details['purpose']}\")\n        print(f\"  Parameters: {details['parameters']}\")\n        print(f\"  Returns: {details['returns']}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0c07de55-22ad-4cef-a46b-b7d46cf149c6",
      "cell_type": "code",
      "source": "import json\nimport time\nfrom time import sleep\nfrom datetime import datetime\nfrom typing import Optional, Tuple, Any, Dict\n\nimport pandas as pd\nfrom tqdm import tqdm\nimport sempy.fabric as fabric\n\n\n# ----------------------------\n# Helpers\n# ----------------------------\n\ndef is_dev_workspace() -> bool:\n    workspace_id = fabric.get_workspace_id()\n    workspace_name = fabric.resolve_workspace_name(workspace_id)\n    return \"dev\" in (workspace_name or \"\").lower()\n\n\ndef _safe_get(d: Dict[str, Any], *paths, default=None):\n    \"\"\"\n    Safely traverse nested dicts/lists to extract a value.\n    Example: _safe_get(obj, 'a', 0, 'b') -> obj['a'][0]['b'] if present, else default.\n    \"\"\"\n    cur = d\n    try:\n        for p in paths:\n            if isinstance(cur, list) and isinstance(p, int):\n                cur = cur[p]\n            elif isinstance(cur, dict):\n                cur = cur.get(p)\n            else:\n                return default\n        return cur if cur is not None else default\n    except Exception:\n        return default\n\n\ndef _extract_failure_info(payload: Dict[str, Any]) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Try a variety of common keys/paths that might contain failure reason/message/details.\n    Returns (failure_reason, error_message, details_blob)\n    \"\"\"\n    candidate_reason_keys = [\n        \"failureReason\", \"Failure Reason\", \"reason\", \"statusReason\", \"cancellationReason\"\n    ]\n    candidate_message_keys = [\n        \"errorMessage\", \"Error Message\", \"message\", \"statusMessage\", \"failureMessage\", \"exceptionMessage\", \"detail\"\n    ]\n\n    # Flat lookups\n    reason = None\n    for k in candidate_reason_keys:\n        v = payload.get(k)\n        if v:\n            reason = v\n            break\n\n    message = None\n    for k in candidate_message_keys:\n        v = payload.get(k)\n        if v:\n            message = v\n            break\n\n    # Nested common shapes\n    if not reason:\n        reason = _safe_get(payload, \"error\", \"code\")\n    if not message:\n        message = _safe_get(payload, \"error\", \"message\") or _safe_get(payload, \"statusDetails\", \"message\")\n\n    # Some services include arrays of errors\n    if not message:\n        errs = _safe_get(payload, \"errors\") or _safe_get(payload, \"error\", \"details\")\n        if isinstance(errs, list) and errs:\n            message = errs[0].get(\"message\") or errs[0].get(\"detail\") or str(errs[0])\n\n    # Compact details blob (pretty JSON) so you have raw material for debugging\n    try:\n        details_blob = json.dumps(payload, indent=2, ensure_ascii=False)\n    except Exception:\n        details_blob = None\n\n    return reason, message, details_blob\n\n\ndef format_datetime(dt):\n    if pd.isna(dt):\n        return \"N/A\"\n    if isinstance(dt, str):\n        try:\n            return datetime.fromisoformat(dt).strftime('%Y-%m-%d %H:%M:%S')\n        except ValueError:\n            return dt\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%d %H:%M:%S')\n    return str(dt)\n\n\n# ----------------------------\n# Fabric Job Runner (Pipelines & Notebooks)\n# ----------------------------\n\ndef _run_fabric_job(\n    artifact_name: str,\n    job_type: str,\n    workspace_name: str = None,\n    poll_interval: int = 10\n) -> Optional[str]:\n    \"\"\"\n    Triggers and polls a Fabric job (Pipeline or RunNotebook).\n    On failure, attempts to print failure reason and message from the job instance payload.\n    Returns job_id on success, else None.\n    \"\"\"\n    client = fabric.FabricRestClient()\n\n    if workspace_name:\n        workspace_id = fabric.resolve_workspace_id(workspace_name)\n    else:\n        workspace_id = fabric.get_workspace_id()\n\n    item_id = fabric.resolve_item_id(artifact_name, workspace=workspace_id)\n\n    supported_job_types = {\"Pipeline\", \"RunNotebook\"}\n    if job_type not in supported_job_types:\n        print(f\"Unsupported job type: {job_type}. Supported types are: {supported_job_types}\")\n        return None\n\n    url = f\"/v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances?jobType={job_type}\"\n\n    # Start job\n    if job_type == \"RunNotebook\":\n        response = client.post(url, json={\"executionData\": {}})\n    else:\n        response = client.post(url)\n\n    if response.status_code != 202:\n        print(f\"{job_type} '{artifact_name}' failed to start. \"\n              f\"Status code: {response.status_code}, {response.text}\")\n        return None\n\n    job_id = response.headers[\"Location\"].split(\"/\")[-1]\n    print(f\"ðŸ”„ {job_type} '{artifact_name}' triggered. Job ID: {job_id}\")\n\n    status_url = f\"/v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances/{job_id}\"\n    last_status = None\n    start_ts = time.time()\n\n    time.sleep(poll_interval)\n\n    while True:\n        status_response = client.get(status_url)\n\n        if status_response.status_code != 200:\n            print(f\"âš ï¸ Unexpected status code while polling: {status_response.status_code}\")\n            time.sleep(poll_interval)\n            continue\n\n        # Parse payload\n        try:\n            status_data = status_response.json()\n            if isinstance(status_data, str):\n                status_data = json.loads(status_data)\n        except Exception as e:\n            print(f\"Failed to parse job status: {e}\")\n            print(f\"Raw: {status_response.text}\")\n            time.sleep(poll_interval)\n            continue\n\n        status = status_data.get(\"status\", \"Unknown\")\n        if status != last_status:\n            print(f\"â±ï¸ Job Status: {status}\")\n            last_status = status\n\n        if status == \"Completed\":\n            elapsed = round(time.time() - start_ts, 2)\n            print(f\"âœ… {job_type} '{artifact_name}' completed in {elapsed} seconds.\")\n            return job_id\n\n        if status in (\"Failed\", \"Cancelled\"):\n            elapsed = round(time.time() - start_ts, 2)\n            print(f\"âŒ {job_type} '{artifact_name}' did not complete. Status: {status}\")\n            # Try to extract reason/message\n            reason, message, details_blob = _extract_failure_info(status_data)\n\n            # Sometimes there are run-level logs/URIs\n            log_uri = (\n                _safe_get(status_data, \"logUri\")\n                or _safe_get(status_data, \"logsUri\")\n                or _safe_get(status_data, \"output\", \"logUri\")\n            )\n\n            print(\"   âž¡ï¸ Failure Reason:\", reason or \"N/A\")\n            print(\"   âž¡ï¸ Error Message :\", message or \"N/A\")\n            if log_uri:\n                print(\"   ðŸ“„ Logs URI      :\", log_uri)\n            print(f\"   â±ï¸ Duration      : {elapsed} seconds.\")\n\n            # If nothing specific was found, dump a trimmed details blob (last resort)\n            if not (reason or message) and details_blob:\n                # Keep it compact to avoid overwhelming logs\n                preview = details_blob[:2000] + (\" â€¦(truncated)â€¦\" if len(details_blob) > 2000 else \"\")\n                print(\"   ðŸ”Ž Raw Payload (preview):\")\n                print(preview)\n            return None\n\n        time.sleep(poll_interval)\n\n\ndef run_notebook(artifact_name: str, workspace_name: Optional[str] = None, poll_interval: int = 10) -> Optional[str]:\n    return _run_fabric_job(\n        artifact_name, job_type=\"RunNotebook\", workspace_name=workspace_name, poll_interval=poll_interval\n    )\n\n\ndef run_pipeline(artifact_name: str, workspace_name: Optional[str] = None, poll_interval: int = 10) -> Optional[str]:\n    return _run_fabric_job(\n        artifact_name, job_type=\"Pipeline\", workspace_name=workspace_name, poll_interval=poll_interval\n    )\n\n\n# ----------------------------\n# Dataset Refresh Runner\n# ----------------------------\n\ndef refresh_dataset(dataset: str, workspace: str, poll_interval: int = 10) -> Optional[str]:\n    \"\"\"\n    Triggers and polls a dataset refresh. Prints detailed failure info when status != Completed.\n    Returns the Refresh Request ID (if obtainable) on success, else None.\n    \"\"\"\n    print(f\"ðŸ”„ Starting dataset refresh: '{dataset}' in workspace '{workspace}'\")\n\n    try:\n        fabric.refresh_dataset(dataset, workspace)\n    except Exception as e:\n        print(f\"âŒ Failed to trigger dataset refresh: {e}\")\n        return None\n\n    last_status = None\n    start_wall = time.time()\n\n    time.sleep(poll_interval)\n\n    refresh_request_id = None\n\n    while True:\n        try:\n            df = fabric.list_refresh_requests(dataset=dataset, workspace=workspace)\n            if df is None or len(df.index) == 0:\n                raise RuntimeError(\"No refresh requests found.\")\n            latest_request = df.iloc[0]\n        except Exception as e:\n            print(f\"âš ï¸ Failed to fetch refresh request: {e}\")\n            time.sleep(poll_interval)\n            continue\n\n        status = latest_request.get('Status', 'Unknown')\n        start = latest_request.get('Start Time')\n        end = latest_request.get('End Time')\n\n        # Capture request ID if present\n        refresh_request_id = latest_request.get('Id') or latest_request.get('ID') or refresh_request_id\n\n        if status != last_status:\n            print(f\"â±ï¸ Refresh Status: {status} | Start: {format_datetime(start)} | End: {format_datetime(end)}\")\n            last_status = status\n\n        if status in ['Completed', 'Failed', 'Cancelled']:\n            try:\n                elapsed = round((end - start).total_seconds(), 2)\n            except Exception:\n                elapsed = round(time.time() - start_wall, 2)\n\n            if status == 'Completed':\n                print(f\"âœ… Dataset '{dataset}' refresh completed in {elapsed} seconds.\")\n                if refresh_request_id:\n                    print(f\"   ðŸ†” Refresh Request ID: {refresh_request_id}\")\n                return refresh_request_id\n\n            # Extract failure columns if available\n            failure_reason = latest_request.get('Failure Reason') or latest_request.get('Reason') or None\n            error_message = latest_request.get('Error Message') or latest_request.get('Message') or None\n\n            # Build a dict for a raw row preview if we don't have clear fields\n            row_dict = {}\n            try:\n                row_dict = latest_request.to_dict()\n            except Exception:\n                pass\n\n            print(f\"âŒ Dataset '{dataset}' refresh did not complete successfully.\")\n            print(f\"   âž¡ï¸ Status         : {status}\")\n            print(f\"   âž¡ï¸ Failure Reason : {failure_reason or 'N/A'}\")\n            print(f\"   âž¡ï¸ Error Message  : {error_message or 'N/A'}\")\n            if refresh_request_id:\n                print(f\"   ðŸ†” Refresh Request ID: {refresh_request_id}\")\n            print(f\"   â±ï¸ Duration       : {elapsed} seconds.\")\n\n            # If neither failure_reason nor error_message present, dump a compact preview of the row\n            if not (failure_reason or error_message) and row_dict:\n                try:\n                    blob = json.dumps(row_dict, indent=2, ensure_ascii=False)\n                    preview = blob[:2000] + (\" â€¦(truncated)â€¦\" if len(blob) > 2000 else \"\")\n                    print(\"   ðŸ”Ž Latest Request Row (preview):\")\n                    print(preview)\n                except Exception:\n                    pass\n\n            return None\n\n        time.sleep(poll_interval)\n\n\n# ----------------------------\n# Example usage (uncomment to run)\n# ----------------------------\n\n# if __name__ == \"__main__\":\n#     # Pipelines / Notebooks\n#     run_pipeline(\"My Pipeline\", workspace_name=\"My Workspace\", poll_interval=10)\n#     run_notebook(\"My Notebook\", workspace_name=\"My Workspace\", poll_interval=10)\n#\n#     # Dataset refresh\n#     refresh_dataset(\"My Dataset\", workspace=\"My Workspace\", poll_interval=10)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8a6d9b68-c01e-4406-947e-8547db88fd82",
      "cell_type": "code",
      "source": "def get_gold_workspace():\n    import sempy.fabric as fabric\n    curr_workspace = fabric.get_notebook_workspace_id()  # Get the current workspace ID\n    workspaces = fabric.list_workspaces()  # List all available workspaces\n    curr_workspace_item = workspaces[workspaces.Id == curr_workspace]  # Find current workspace details\n    lakehouse_storage = curr_workspace_item['Name'].iloc[0]  # Get the workspace name\n    \n    # Mapping of workspaces to their respective Silver lakehouse paths\n    workspace_lakehouse_dict = {\n            'DevSecure_Enclave': 'DevGold', \n            'Secure_Enclave': 'Data and Analytics'\n    }\n\n    return workspace_lakehouse_dict.get(lakehouse_storage, None)  # Return the appropriate path or None if not found",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}