{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0e214d16-d8b3-47ed-8fa2-50dec0036d24",
      "cell_type": "markdown",
      "source": "# DAY 06 - Read/Write to Lakehouse Tables\n- Youtube Link: https://www.youtube.com/watch?v=02lSlhwLU4c",
      "metadata": {}
    },
    {
      "id": "022c9217-234c-40ba-8cee-1d1b6cd0922f",
      "cell_type": "markdown",
      "source": "### Writing from DataFrame to Lakehouse Table",
      "metadata": {}
    },
    {
      "id": "68ea27eb-3f85-4e31-9837-30232420ff92",
      "cell_type": "code",
      "source": "# Get the data\ndf = spark.read.json('Files/json/property-sales.json')\n\ndisplay(df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3f340d29-6a54-428b-a429-c69634ce2e01",
      "cell_type": "markdown",
      "source": "### Beware of Limitations of Lakehouse Column Naming\n- ##### No Special Characters\n  Allowed: Letters, Numbers, Underscore\n- ##### No Spaces\n- ##### Case Sensitivity Differences\n  Spark is 'case-sensitive'.\n  SQL Endpoint is 'case-insensitive'.\n- ##### Cannot Use SQL Reserved Keywords\n  Ex: SELECT, GROUP, ORDER, JOIN, DATE, TABLE, INDEX, NULL\n- ##### Cannot Start with a Number\n- ##### Column Names Must be Unique\n  Fabric does not allow duplicate names, even with different casing.\n- ##### Avoid Leading and Trailing Underscores\n  These may cause issues during schema inference.\n- ##### Renaming Columns in Delta Tables Is Limited\n  Once written, column names are not easy to rename. May require: recreating the table, rewriting data with a new schema\n- ##### Avoid Very Long Column Names\n- ##### No Duplicate Column Names Across Joins Without Aliasing\n  Spark will enforce unique names when joining tables â€” duplicates must be renamed.",
      "metadata": {}
    },
    {
      "id": "8f626e48-a762-4029-81bd-92dee3cb7b86",
      "cell_type": "markdown",
      "source": "### Inspecting the Schema",
      "metadata": {}
    },
    {
      "id": "343b9a06-abba-4f0e-8e3a-3e279dfc6a74",
      "cell_type": "code",
      "source": "df.printSchema()\n\n# output: (nullable = true) means that the column can have null values",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "93838d68-624d-4ddd-9332-0cc7582649b9",
      "cell_type": "markdown",
      "source": "### Renaming Columns",
      "metadata": {}
    },
    {
      "id": "3cd812fb-814d-41f5-8f6c-ed58f7e0ce80",
      "cell_type": "code",
      "source": "# Changing column names before writing to Lakehouse Tables\ndf = df.withColumnRenamed(\"ColumnName\", \"NewColumnName\")\n\n# Example:\ndf = df.withColumnRenamed(\"SalePrice ($)\", \"SalePrice_USD\")\\\n        .withColumnRenamed(\"Address \", \"Address\")\\\n        .withColumnRenamed(\"City \", \"City\")\n\n# Check if the renaming is successful\ndf.printSchema()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bbfa9ce6-254e-4e7e-8602-e16de0f320f1",
      "cell_type": "markdown",
      "source": "### Writing DF to Table, with different \"Modes\"",
      "metadata": {}
    },
    {
      "id": "77a86ee5-f06f-444d-a32c-88f4c196f979",
      "cell_type": "markdown",
      "source": "### Managed Table",
      "metadata": {}
    },
    {
      "id": "3e20de65-ccb0-49de-bb12-7c24f88d2e80",
      "cell_type": "code",
      "source": "delta_table_name = 'PropertySales'\n\n# Use saveAsTable to save as a Managed Table\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a8a60bc3-28c4-482a-b1a8-8313bc202067",
      "cell_type": "markdown",
      "source": "#### Four (4) Different Write Modes\n- Append\n- Overwrite\n- Error\n- Ignore",
      "metadata": {}
    },
    {
      "id": "6c166847-7f3b-4ee2-a7e6-7378d47ae234",
      "cell_type": "code",
      "source": "# These are the 4 different write 'modes'\n\n# Append the new dataframe to an existing table\ndf.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\n\n# Overwrite existing table with the new dataframe\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n\n# Throw error if data already exists\ndf.write.mode(\"error\").format(\"delta\").saveAsTable(delta_table_name)\n\n# Fail silently if data already exists\ndf.write.mode(\"ignore\").format(\"delta\").saveAsTable(delta_table_name)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "383ad686-8b82-427d-ae51-bc97c03c2ebb",
      "cell_type": "markdown",
      "source": "### Write to an Unmanaged Table\n- for export to external file system/Databricks/Snowflake",
      "metadata": {}
    },
    {
      "id": "2c25f48c-b9f7-4c00-9a54-8a1b923110e9",
      "cell_type": "code",
      "source": "# Unmanaged Table\ndf.write.mode(\"overwrite\").format(\"delta\").save(path = \"Files/delta/unmanaged.delta\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2b04a72f-6b82-40a0-9589-e540906b0e22",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}