{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0dcebac8-274b-4891-b60f-cada4095155f",
      "cell_type": "markdown",
      "source": "# DAY 05 - Read Files into DataFrame\n- Youtube Link: https://www.youtube.com/watch?v=02lSlhwLU4c",
      "metadata": {}
    },
    {
      "id": "1a68d84d-570e-4959-a708-5e3ba8c7d220",
      "cell_type": "markdown",
      "source": "### Reading a CSV file into a Spark DataFrame\n- Basic read\n- with headers,\n- inferring the Schema",
      "metadata": {}
    },
    {
      "id": "7ebfe4fa-ef05-4111-a3b2-cf83381381e2",
      "cell_type": "code",
      "source": "# Declare the path to the file\ncsv_path = 'Files/property-sales.csv'\n\n# Read a CSV file from Files/property-sales.csv\ndf_csv = spark.read.csv(csv_path, headers = True, inferSchema = True)\n\ndisplay(df_csv)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0a76e59e-3f6f-4820-986a-e254d4aff60e",
      "cell_type": "code",
      "source": "df_csv.dtypes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4c9f1ec8-2d8a-4c62-813c-fc61d9156372",
      "cell_type": "markdown",
      "source": "### Writing DataFrames to files (JSON)\n- We can write our dataframe as a JSON file by calling df.write.json()",
      "metadata": {}
    },
    {
      "id": "611efb22-aafb-4cad-b691-063316d3e654",
      "cell_type": "code",
      "source": "# Call write.json()\ndf_csv.write.json(\"Files/json/property-sales.json\", mode = 'overwrite')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e7fe6cbc-b1b0-4f89-94ff-8fcc3c52747a",
      "cell_type": "markdown",
      "source": "### Reading a JSON File into DataFrame",
      "metadata": {}
    },
    {
      "id": "58e1760b-08e9-493e-9044-b4d0d96389f2",
      "cell_type": "code",
      "source": "df_json = spark.read.json('Files/json/property-sales.json')\ndisplay(df_json)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7495f4a-83ef-4b24-ac4d-53c5be9868f5",
      "cell_type": "markdown",
      "source": "### Writing out to Parquet",
      "metadata": {}
    },
    {
      "id": "e9e5d972-4581-4622-8fa5-5cdca2a027b2",
      "cell_type": "code",
      "source": "df_json.write.parquet('Files/json/property-sales.parquet', mode = 'overwrite')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a0e20a43-b4b8-47f0-9320-f1f1a4b0ec7b",
      "cell_type": "markdown",
      "source": "### Reading a Parquet into a DataFrame",
      "metadata": {}
    },
    {
      "id": "84097e8d-3d09-4c01-ae75-3b0f8b23b5f6",
      "cell_type": "code",
      "source": "df_parquet = spark.read.parquet('Files/json/property-sales.parquet')\ndisplay(df_parquet)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f1e8f95b-5267-4444-a0dc-a12a322166d9",
      "cell_type": "markdown",
      "source": "# Reading Multiple Files in the Same Folder\n- creating multiple parquet files in the parquet subfolder first\n- read in all the parquet files into one df",
      "metadata": {}
    },
    {
      "id": "76c68d5f-72e3-4603-8bf3-a51bc565266d",
      "cell_type": "code",
      "source": "# Read all the parquet files in the 'Files/parquet/' folder into a dataframe\ndf_all_parquet = spark.read.parquet('Files/parquet/*.parquet')    # \"*\" wildcard symbol that will read all of the files within the folder",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0276e906-a0be-4672-bd22-d9bb5f78241a",
      "cell_type": "markdown",
      "source": "### Checking if this has worked using _metadata\n- Spark provides us with all the file metadata in a \"hidden\" column that we can add to the dataframe using _metadata.",
      "metadata": {}
    },
    {
      "id": "edeab10e-44d9-4a42-b486-e39587ba7e40",
      "cell_type": "code",
      "source": "# Read all the parquet files, then add the _metadata column\ndf_all_parquet_plus_metadata = spark.read\\     # \"\\\" cutting the code - go to a new line to make it more readable\n    .parquet('Files/parquet/*.parquet')\\\n    .select(\"*\", \"_metadata\")\n\ndisplay(df_all_parquet_plus_metadata)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6fec8f08-7bb1-4166-ae86-e2dc1319566c",
      "cell_type": "markdown",
      "source": "# Further Learning\n- Ignoring corrupt/missing files\n- Custom path filtering (PathGlobFilter)\n- More recursive file reading patterns within complex folder structures.",
      "metadata": {}
    },
    {
      "id": "83f69a0a-1749-4864-92b0-eb2180a08922",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}