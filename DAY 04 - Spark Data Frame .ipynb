{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f173402a-f429-4a22-a305-727280405817",
      "cell_type": "markdown",
      "source": "# DAY 4: Spark Data Frame \n- Youtube Link: https://www.youtube.com/watch?v=02lSlhwLU4c",
      "metadata": {}
    },
    {
      "id": "b3c9e676-0807-4f97-a14e-8d7c177cd2dc",
      "cell_type": "markdown",
      "source": "### What is a Spark Data Frame?\n- A Spark DataFrame is like a distributed, in-memory table with named columns and schema.\n- The Schema defines the columns and the data types for each column.\n- Inspired by Pandas DataFrames!",
      "metadata": {}
    },
    {
      "id": "f961b54b-f970-4aed-b904-c18eb06583a8",
      "cell_type": "markdown",
      "source": "### Creating our first data frame (with a schema)",
      "metadata": {}
    },
    {
      "id": "4afe9ae2-8eee-4d5f-9a57-f5918ca797e7",
      "cell_type": "code",
      "source": "from pyspark.sql.types\nimport StructType, StructField, StringType, IntegerType\n\ndata2 = [(\"Jack\", \"\", \"Eldridge\", \"12345\", \"M\", 90000),\n        (\"Elliot\", \"Jordan\", \"Monro\", \"12346\", \"M\", 47000),\n        (\"Sandra\", \"Faye\", \"Roberts\", \"12347\", \"F\", 95000),\n        (\"Anne\", \"Oway\", \"Jones\", \"12348\", \"F\", 78000)]\n\nschema = StructType([\n  StructField(\"firstname\", StringType(), True),\n  StructField(\"middlename\", StringType(), True),\n  StructField(\"lastname\", StringType(), True),\n  StructField(\"id\", StringType(), True),\n  StructField(\"gender\", StringType(), True),\n  StructField(\"salary\", IntegerType(), True)\n])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de56d025-89b4-4716-8e68-8483d56771ba",
      "cell_type": "code",
      "source": "df = spark.createDataFrame(data = data2, schema = schema)\ndf.printSchema()\ndf.show(truncate = False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b6aca07e-5e1c-4c1a-9477-22905ea75ec5",
      "cell_type": "code",
      "source": "type(df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3dd56183-dce6-4bdd-9361-3023b8169c5e",
      "cell_type": "markdown",
      "source": "### Why Schemas?\n- commonly used, especially with reading from an external data source (including files)\n- Spark doesn't have to 'infer' the data type (which can be expensive for money, time, and processing power)\n- you can detect errors early if the data doesn't match the schema",
      "metadata": {}
    },
    {
      "id": "f29044b6-77dc-488b-8033-5b3a123ff6b1",
      "cell_type": "markdown",
      "source": "### Defining a Schema using Data Definition Language (DDL)",
      "metadata": {}
    },
    {
      "id": "a9aaee4d-448e-4790-8d22-f5b344d08734",
      "cell_type": "code",
      "source": "schema_ddl = \"firstname STRING, middlename STRING, lastname STRING, id STRING, gender STRING, salary INT\" \ndf_with_ddl_schema = spark.createDataFrame(data = data2, schema = schema_ddl)\ndf.printSchema()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}